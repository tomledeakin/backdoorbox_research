Creating poisoned training set for WaNet on cifar10...
[target class : 0]
Files already downloaded and verified
Poison indices: [6, 116, 244, 495, 639, 643, 777, 919, 1478, 1528, 1537, 1641, 1687, 1730, 1854, 1878, 1881, 2370, 2516, 2530, 2978, 3035, 3102, 3616, 4017, 4384, 4537, 4771, 4827, 4949, 5109, 5160, 5380, 5389, 5420, 5616, 5930, 6002, 6043, 6128, 6328, 6335, 6432, 6463, 6690, 6743, 6783, 7018, 7098, 7267, 7393, 7421, 7462, 7492, 7502, 7580, 7684, 7775, 8038, 8088, 8101, 8152, 8170, 8254, 8312, 8396, 8420, 8510, 8606, 8625, 8988, 9026, 9096, 9322, 9345, 9362, 9408, 9412, 9434, 9840, 9863, 9895, 10010, 10011, 10022, 10032, 10147, 10154, 10310, 10371, 10417, 10595, 10720, 10733, 10754, 11146, 11253, 11263, 11294, 11296, 11350, 11420, 11424, 11655, 11831, 11895, 11958, 11973, 11995, 12173, 12229, 12523, 12561, 12645, 12691, 12730, 12804, 12931, 12948, 13132, 13315, 13448, 13480, 13640, 13699, 13704, 13811, 13877, 14005, 14018, 14053, 14123, 14231, 14404, 14493, 14549, 14650, 14776, 14833, 14950, 15039, 15195, 15321, 15326, 15330, 15876, 15886, 15920, 16017, 16020, 16087, 16116, 16268, 16270, 16280, 16385, 16520, 16585, 16643, 16853, 16893, 17205, 17248, 17348, 17503, 17532, 17766, 17824, 18065, 18132, 18493, 18591, 18701, 18703, 18795, 18843, 18910, 18937, 19018, 19109, 19186, 19202, 19450, 19453, 19456, 19503, 19823, 19881, 19887, 19970, 20073, 20135, 20232, 20388, 20465, 20600, 20980, 20997, 21377, 21440, 21555, 21816, 21834, 21849, 22016, 22018, 22019, 22045, 22238, 22248, 22304, 22336, 22360, 22392, 22785, 22935, 22996, 23231, 23247, 23253, 23290, 23396, 23523, 23574, 23662, 23860, 23903, 23994, 24111, 24147, 24299, 24342, 24442, 24570, 24801, 24917, 24961, 25035, 25085, 25183, 25279, 25300, 25304, 25435, 25520, 25554, 25815, 25957, 26207, 26237, 26301, 26360, 26390, 26464, 26605, 26668, 26773, 26930, 26996, 27651, 27688, 27696, 27710, 27902, 28123, 28159, 28283, 28338, 28346, 28382, 28384, 28412, 28470, 28612, 28649, 28734, 28785, 28821, 28872, 29210, 29235, 29463, 29626, 29693, 29899, 29996, 30051, 30089, 30167, 30479, 30491, 30544, 30559, 30586, 30587, 30628, 30770, 30851, 30878, 30931, 31004, 31035, 31041, 31075, 31268, 31277, 31357, 31570, 31572, 31610, 31815, 32207, 32324, 32352, 32388, 32493, 32572, 32584, 32972, 33115, 33243, 33393, 33405, 33411, 33424, 33522, 33664, 33680, 33727, 33734, 33788, 33806, 33879, 33938, 34210, 34286, 34290, 34305, 34307, 34380, 34392, 34660, 34665, 34683, 35026, 35087, 35143, 35167, 35217, 35241, 35319, 35399, 35410, 35609, 35625, 35689, 35756, 36055, 36098, 36273, 36298, 36382, 36465, 36481, 36509, 36585, 36677, 36824, 36832, 37105, 37127, 37144, 37177, 37179, 37544, 37576, 37771, 37822, 38051, 38095, 38108, 38130, 38144, 38394, 38450, 38521, 38641, 38756, 38806, 38983, 39126, 39168, 39185, 39244, 39281, 39289, 39416, 39440, 39557, 39577, 39584, 39663, 39675, 39800, 39916, 39991, 40241, 40309, 40407, 40598, 40600, 40616, 40646, 40748, 40795, 40890, 40974, 40975, 41036, 41071, 41236, 41321, 41458, 41708, 41805, 41921, 42009, 42071, 42176, 42282, 42355, 42359, 42404, 42812, 43356, 43378, 43496, 43525, 43540, 43592, 43612, 44561, 44571, 44594, 44682, 44716, 44768, 44850, 44895, 44993, 45015, 45036, 45049, 45254, 45274, 45282, 45524, 45534, 45578, 45821, 45924, 46062, 46374, 46598, 46602, 46721, 46865, 47034, 47061, 47068, 47122, 47139, 47245, 47268, 47428, 47813, 47904, 47981, 47996, 48071, 48072, 48167, 48178, 48271, 48314, 48356, 48412, 48416, 48570, 48757, 48900, 49116, 49146, 49208, 49444, 49525, 49592, 49703, 49770, 49882]
Cover indices: []
[Generate Poisoned Set] Save 50000 Images
[Generate Poisoned Set] Save poisoned_train_set/cifar10/WaNet_0.010_cover=0.000_poison_seed=0/cover_indices
[Generate Poisoned Set] Save poisoned_train_set/cifar10/WaNet_0.010_cover=0.000_poison_seed=0/imgs
[Generate Poisoned Set] Save poisoned_train_set/cifar10/WaNet_0.010_cover=0.000_poison_seed=0/labels
[Generate Poisoned Set] Save poisoned_train_set/cifar10/WaNet_0.010_cover=0.000_poison_seed=0/poison_indices
Training the model on the poisoned dataset...
dataset : poisoned_train_set/cifar10/WaNet_0.010_cover=0.000_poison_seed=0/imgs
Will save to 'poisoned_train_set/cifar10/WaNet_0.010_cover=0.000_poison_seed=0/full_base_aug_seed=2333.pt'.
<Backdoor Training> Train Epoch: 1 	Loss: 1.791733, lr: 0.100000, Time: 10.66s
Clean ACC: 3509/8000 = 0.438625, Loss: 1.499618649482727
ASR: 315/7193 = 0.043793


<Backdoor Training> Train Epoch: 2 	Loss: 1.342511, lr: 0.100000, Time: 6.32s
Clean ACC: 4255/8000 = 0.531875, Loss: 1.2576547861099243
ASR: 299/7193 = 0.041568


<Backdoor Training> Train Epoch: 3 	Loss: 1.135110, lr: 0.100000, Time: 6.24s
Clean ACC: 4778/8000 = 0.597250, Loss: 1.12114679813385
ASR: 215/7193 = 0.029890


<Backdoor Training> Train Epoch: 4 	Loss: 0.853056, lr: 0.100000, Time: 6.17s
Clean ACC: 5500/8000 = 0.687500, Loss: 0.8828941583633423
ASR: 187/7193 = 0.025997


<Backdoor Training> Train Epoch: 5 	Loss: 0.813193, lr: 0.100000, Time: 6.32s
Clean ACC: 5789/8000 = 0.723625, Loss: 0.7774231433868408
ASR: 127/7193 = 0.017656


<Backdoor Training> Train Epoch: 6 	Loss: 0.577570, lr: 0.100000, Time: 6.34s
Clean ACC: 5663/8000 = 0.707875, Loss: 0.8744379281997681
ASR: 97/7193 = 0.013485


<Backdoor Training> Train Epoch: 7 	Loss: 0.639608, lr: 0.100000, Time: 6.25s
Clean ACC: 6227/8000 = 0.778375, Loss: 0.6652980446815491
ASR: 73/7193 = 0.010149


<Backdoor Training> Train Epoch: 8 	Loss: 0.382695, lr: 0.100000, Time: 6.40s
Clean ACC: 6211/8000 = 0.776375, Loss: 0.6712960004806519
ASR: 399/7193 = 0.055471


<Backdoor Training> Train Epoch: 9 	Loss: 0.527316, lr: 0.100000, Time: 6.70s
Clean ACC: 6511/8000 = 0.813875, Loss: 0.5468223690986633
ASR: 147/7193 = 0.020437


<Backdoor Training> Train Epoch: 10 	Loss: 0.379881, lr: 0.100000, Time: 6.79s
Clean ACC: 6604/8000 = 0.825500, Loss: 0.5166729688644409
ASR: 263/7193 = 0.036563


<Backdoor Training> Train Epoch: 11 	Loss: 0.550212, lr: 0.100000, Time: 6.64s
Clean ACC: 6517/8000 = 0.814625, Loss: 0.5633804798126221
ASR: 170/7193 = 0.023634


<Backdoor Training> Train Epoch: 12 	Loss: 0.298275, lr: 0.100000, Time: 6.76s
Clean ACC: 6725/8000 = 0.840625, Loss: 0.4839286208152771
ASR: 107/7193 = 0.014876


<Backdoor Training> Train Epoch: 13 	Loss: 0.552355, lr: 0.100000, Time: 6.70s
Clean ACC: 6674/8000 = 0.834250, Loss: 0.4878232181072235
ASR: 169/7193 = 0.023495


<Backdoor Training> Train Epoch: 14 	Loss: 0.406043, lr: 0.100000, Time: 6.36s
Clean ACC: 6872/8000 = 0.859000, Loss: 0.4316503703594208
ASR: 281/7193 = 0.039066


<Backdoor Training> Train Epoch: 15 	Loss: 0.402728, lr: 0.100000, Time: 6.21s
Clean ACC: 6788/8000 = 0.848500, Loss: 0.46809008717536926
ASR: 288/7193 = 0.040039


<Backdoor Training> Train Epoch: 16 	Loss: 0.171497, lr: 0.100000, Time: 6.25s
Clean ACC: 6920/8000 = 0.865000, Loss: 0.41778120398521423
ASR: 217/7193 = 0.030168


<Backdoor Training> Train Epoch: 17 	Loss: 0.361310, lr: 0.100000, Time: 6.05s
Clean ACC: 6947/8000 = 0.868375, Loss: 0.40178924798965454
ASR: 312/7193 = 0.043376


<Backdoor Training> Train Epoch: 18 	Loss: 0.235967, lr: 0.100000, Time: 6.25s
Clean ACC: 6916/8000 = 0.864500, Loss: 0.42858660221099854
ASR: 241/7193 = 0.033505


<Backdoor Training> Train Epoch: 19 	Loss: 0.450933, lr: 0.100000, Time: 6.28s
Clean ACC: 6967/8000 = 0.870875, Loss: 0.396842360496521
ASR: 260/7193 = 0.036146


<Backdoor Training> Train Epoch: 20 	Loss: 0.320861, lr: 0.100000, Time: 6.27s
Clean ACC: 6992/8000 = 0.874000, Loss: 0.38763707876205444
ASR: 236/7193 = 0.032810


<Backdoor Training> Train Epoch: 21 	Loss: 0.324599, lr: 0.100000, Time: 6.63s
Clean ACC: 7044/8000 = 0.880500, Loss: 0.3747502267360687
ASR: 293/7193 = 0.040734


<Backdoor Training> Train Epoch: 22 	Loss: 0.178723, lr: 0.100000, Time: 6.76s
Clean ACC: 7033/8000 = 0.879125, Loss: 0.3784594237804413
ASR: 187/7193 = 0.025997


<Backdoor Training> Train Epoch: 23 	Loss: 0.281738, lr: 0.100000, Time: 6.82s
Clean ACC: 6926/8000 = 0.865750, Loss: 0.41477033495903015
ASR: 358/7193 = 0.049771


<Backdoor Training> Train Epoch: 24 	Loss: 0.359277, lr: 0.100000, Time: 6.78s
Clean ACC: 6991/8000 = 0.873875, Loss: 0.39814838767051697
ASR: 245/7193 = 0.034061


<Backdoor Training> Train Epoch: 25 	Loss: 0.256580, lr: 0.100000, Time: 6.88s
Clean ACC: 6986/8000 = 0.873250, Loss: 0.3969160318374634
ASR: 224/7193 = 0.031141


<Backdoor Training> Train Epoch: 26 	Loss: 0.481494, lr: 0.100000, Time: 6.67s
Clean ACC: 6985/8000 = 0.873125, Loss: 0.40505993366241455
ASR: 166/7193 = 0.023078


<Backdoor Training> Train Epoch: 27 	Loss: 0.219665, lr: 0.100000, Time: 6.14s
Clean ACC: 7092/8000 = 0.886500, Loss: 0.36067548394203186
ASR: 558/7193 = 0.077575


<Backdoor Training> Train Epoch: 28 	Loss: 0.239657, lr: 0.100000, Time: 6.15s
Clean ACC: 6865/8000 = 0.858125, Loss: 0.4938937723636627
ASR: 351/7193 = 0.048797


<Backdoor Training> Train Epoch: 29 	Loss: 0.239573, lr: 0.100000, Time: 6.21s
Clean ACC: 7019/8000 = 0.877375, Loss: 0.391173392534256
ASR: 1346/7193 = 0.187126


<Backdoor Training> Train Epoch: 30 	Loss: 0.219831, lr: 0.100000, Time: 6.18s
Clean ACC: 7050/8000 = 0.881250, Loss: 0.3770082890987396
ASR: 689/7193 = 0.095788


<Backdoor Training> Train Epoch: 31 	Loss: 0.176748, lr: 0.100000, Time: 6.30s
Clean ACC: 6815/8000 = 0.851875, Loss: 0.5045939087867737
ASR: 413/7193 = 0.057417


<Backdoor Training> Train Epoch: 32 	Loss: 0.276738, lr: 0.100000, Time: 6.26s
Clean ACC: 6980/8000 = 0.872500, Loss: 0.4158302843570709
ASR: 1506/7193 = 0.209370


<Backdoor Training> Train Epoch: 33 	Loss: 0.176859, lr: 0.100000, Time: 6.37s
Clean ACC: 6851/8000 = 0.856375, Loss: 0.4636184275150299
ASR: 347/7193 = 0.048241


<Backdoor Training> Train Epoch: 34 	Loss: 0.167353, lr: 0.100000, Time: 6.85s
Clean ACC: 7013/8000 = 0.876625, Loss: 0.3972543179988861
ASR: 1152/7193 = 0.160156


<Backdoor Training> Train Epoch: 35 	Loss: 0.159024, lr: 0.100000, Time: 6.88s
Clean ACC: 7077/8000 = 0.884625, Loss: 0.3731725215911865
ASR: 455/7193 = 0.063256


<Backdoor Training> Train Epoch: 36 	Loss: 0.162429, lr: 0.100000, Time: 6.82s
Clean ACC: 7108/8000 = 0.888500, Loss: 0.36803534626960754
ASR: 1193/7193 = 0.165856


<Backdoor Training> Train Epoch: 37 	Loss: 0.140527, lr: 0.100000, Time: 6.89s
Clean ACC: 7001/8000 = 0.875125, Loss: 0.4162557125091553
ASR: 783/7193 = 0.108856


<Backdoor Training> Train Epoch: 38 	Loss: 0.135614, lr: 0.100000, Time: 6.88s
Clean ACC: 7072/8000 = 0.884000, Loss: 0.40220215916633606
ASR: 508/7193 = 0.070624


<Backdoor Training> Train Epoch: 39 	Loss: 0.185756, lr: 0.100000, Time: 6.18s
Clean ACC: 7098/8000 = 0.887250, Loss: 0.36702030897140503
ASR: 897/7193 = 0.124705


<Backdoor Training> Train Epoch: 40 	Loss: 0.194858, lr: 0.100000, Time: 6.15s
Clean ACC: 7077/8000 = 0.884625, Loss: 0.3845527470111847
ASR: 1006/7193 = 0.139858


<Backdoor Training> Train Epoch: 41 	Loss: 0.168265, lr: 0.100000, Time: 6.34s
Clean ACC: 7061/8000 = 0.882625, Loss: 0.3869217336177826
ASR: 1100/7193 = 0.152926


<Backdoor Training> Train Epoch: 42 	Loss: 0.164430, lr: 0.100000, Time: 6.23s
Clean ACC: 7101/8000 = 0.887625, Loss: 0.3758501410484314
ASR: 1524/7193 = 0.211873


<Backdoor Training> Train Epoch: 43 	Loss: 0.075956, lr: 0.100000, Time: 6.24s
Clean ACC: 7097/8000 = 0.887125, Loss: 0.3998418152332306
ASR: 2029/7193 = 0.282080


<Backdoor Training> Train Epoch: 44 	Loss: 0.387242, lr: 0.100000, Time: 6.29s
Clean ACC: 7095/8000 = 0.886875, Loss: 0.38070207834243774
ASR: 469/7193 = 0.065202


<Backdoor Training> Train Epoch: 45 	Loss: 0.143449, lr: 0.100000, Time: 6.29s
Clean ACC: 7060/8000 = 0.882500, Loss: 0.3889690637588501
ASR: 2375/7193 = 0.330182


<Backdoor Training> Train Epoch: 46 	Loss: 0.154146, lr: 0.100000, Time: 6.82s
Clean ACC: 6830/8000 = 0.853750, Loss: 0.4977504312992096
ASR: 3636/7193 = 0.505491


<Backdoor Training> Train Epoch: 47 	Loss: 0.157654, lr: 0.100000, Time: 6.83s
Clean ACC: 7142/8000 = 0.892750, Loss: 0.3603249192237854
ASR: 2539/7193 = 0.352982


<Backdoor Training> Train Epoch: 48 	Loss: 0.309459, lr: 0.100000, Time: 6.94s
Clean ACC: 7051/8000 = 0.881375, Loss: 0.4142186939716339
ASR: 1951/7193 = 0.271236


<Backdoor Training> Train Epoch: 49 	Loss: 0.271064, lr: 0.100000, Time: 6.68s
Clean ACC: 6975/8000 = 0.871875, Loss: 0.42184311151504517
ASR: 3211/7193 = 0.446406


<Backdoor Training> Train Epoch: 50 	Loss: 0.215315, lr: 0.100000, Time: 6.83s
Clean ACC: 7097/8000 = 0.887125, Loss: 0.38717231154441833
ASR: 2920/7193 = 0.405950


<Backdoor Training> Train Epoch: 51 	Loss: 0.040943, lr: 0.010000, Time: 6.13s
Clean ACC: 7405/8000 = 0.925625, Loss: 0.24397332966327667
ASR: 2415/7193 = 0.335743


<Backdoor Training> Train Epoch: 52 	Loss: 0.017085, lr: 0.010000, Time: 6.16s
Clean ACC: 7445/8000 = 0.930625, Loss: 0.24074916541576385
ASR: 2178/7193 = 0.302794


<Backdoor Training> Train Epoch: 53 	Loss: 0.108073, lr: 0.010000, Time: 6.14s
Clean ACC: 7443/8000 = 0.930375, Loss: 0.24610471725463867
ASR: 2768/7193 = 0.384819


<Backdoor Training> Train Epoch: 54 	Loss: 0.058385, lr: 0.010000, Time: 6.19s
Clean ACC: 7462/8000 = 0.932750, Loss: 0.24581024050712585
ASR: 2218/7193 = 0.308355


<Backdoor Training> Train Epoch: 55 	Loss: 0.018775, lr: 0.010000, Time: 6.30s
Clean ACC: 7447/8000 = 0.930875, Loss: 0.24795246124267578
ASR: 2384/7193 = 0.331433


<Backdoor Training> Train Epoch: 56 	Loss: 0.026112, lr: 0.010000, Time: 6.16s
Clean ACC: 7445/8000 = 0.930625, Loss: 0.25108054280281067
ASR: 2449/7193 = 0.340470


<Backdoor Training> Train Epoch: 57 	Loss: 0.025143, lr: 0.010000, Time: 6.29s
Clean ACC: 7460/8000 = 0.932500, Loss: 0.25141575932502747
ASR: 2625/7193 = 0.364938


<Backdoor Training> Train Epoch: 58 	Loss: 0.010215, lr: 0.010000, Time: 6.55s
Clean ACC: 7441/8000 = 0.930125, Loss: 0.2586987316608429
ASR: 2554/7193 = 0.355067


<Backdoor Training> Train Epoch: 59 	Loss: 0.023954, lr: 0.010000, Time: 6.78s
Clean ACC: 7475/8000 = 0.934375, Loss: 0.24967879056930542
ASR: 2220/7193 = 0.308633


<Backdoor Training> Train Epoch: 60 	Loss: 0.005262, lr: 0.010000, Time: 6.80s
Clean ACC: 7461/8000 = 0.932625, Loss: 0.2553536295890808
ASR: 2539/7193 = 0.352982


<Backdoor Training> Train Epoch: 61 	Loss: 0.006918, lr: 0.010000, Time: 6.76s
Clean ACC: 7467/8000 = 0.933375, Loss: 0.2598731815814972
ASR: 2462/7193 = 0.342277


<Backdoor Training> Train Epoch: 62 	Loss: 0.003177, lr: 0.010000, Time: 6.90s
Clean ACC: 7455/8000 = 0.931875, Loss: 0.26466789841651917
ASR: 2566/7193 = 0.356736


<Backdoor Training> Train Epoch: 63 	Loss: 0.023448, lr: 0.010000, Time: 6.63s
Clean ACC: 7476/8000 = 0.934500, Loss: 0.26700788736343384
ASR: 2935/7193 = 0.408036


<Backdoor Training> Train Epoch: 64 	Loss: 0.002029, lr: 0.010000, Time: 6.32s
Clean ACC: 7479/8000 = 0.934875, Loss: 0.26159748435020447
ASR: 2168/7193 = 0.301404


<Backdoor Training> Train Epoch: 65 	Loss: 0.006421, lr: 0.010000, Time: 6.17s
Clean ACC: 7473/8000 = 0.934125, Loss: 0.26533016562461853
ASR: 2299/7193 = 0.319616


<Backdoor Training> Train Epoch: 66 	Loss: 0.011360, lr: 0.010000, Time: 6.21s
Clean ACC: 7476/8000 = 0.934500, Loss: 0.2716452181339264
ASR: 2337/7193 = 0.324899


<Backdoor Training> Train Epoch: 67 	Loss: 0.005354, lr: 0.010000, Time: 6.28s
Clean ACC: 7474/8000 = 0.934250, Loss: 0.2684890925884247
ASR: 2181/7193 = 0.303211


<Backdoor Training> Train Epoch: 68 	Loss: 0.001127, lr: 0.010000, Time: 6.29s
Clean ACC: 7469/8000 = 0.933625, Loss: 0.2691591680049896
ASR: 2414/7193 = 0.335604


<Backdoor Training> Train Epoch: 69 	Loss: 0.022976, lr: 0.010000, Time: 6.38s
Clean ACC: 7468/8000 = 0.933500, Loss: 0.2685280740261078
ASR: 2325/7193 = 0.323231


<Backdoor Training> Train Epoch: 70 	Loss: 0.003550, lr: 0.010000, Time: 6.53s
Clean ACC: 7472/8000 = 0.934000, Loss: 0.2721808850765228
ASR: 2610/7193 = 0.362853


<Backdoor Training> Train Epoch: 71 	Loss: 0.003672, lr: 0.010000, Time: 6.89s
Clean ACC: 7463/8000 = 0.932875, Loss: 0.2744576930999756
ASR: 2354/7193 = 0.327263


<Backdoor Training> Train Epoch: 72 	Loss: 0.003662, lr: 0.010000, Time: 6.87s
Clean ACC: 7470/8000 = 0.933750, Loss: 0.2692049741744995
ASR: 2390/7193 = 0.332267


<Backdoor Training> Train Epoch: 73 	Loss: 0.004398, lr: 0.010000, Time: 6.65s
Clean ACC: 7475/8000 = 0.934375, Loss: 0.2750922739505768
ASR: 2403/7193 = 0.334075


<Backdoor Training> Train Epoch: 74 	Loss: 0.006200, lr: 0.010000, Time: 6.96s
Clean ACC: 7465/8000 = 0.933125, Loss: 0.2768871486186981
ASR: 2379/7193 = 0.330738


<Backdoor Training> Train Epoch: 75 	Loss: 0.010084, lr: 0.010000, Time: 6.74s
Clean ACC: 7473/8000 = 0.934125, Loss: 0.28231239318847656
ASR: 3113/7193 = 0.432782


<Backdoor Training> Train Epoch: 76 	Loss: 0.000736, lr: 0.001000, Time: 6.27s
Clean ACC: 7470/8000 = 0.933750, Loss: 0.27687564492225647
ASR: 2664/7193 = 0.370360


<Backdoor Training> Train Epoch: 77 	Loss: 0.025671, lr: 0.001000, Time: 6.16s
Clean ACC: 7483/8000 = 0.935375, Loss: 0.27399855852127075
ASR: 2333/7193 = 0.324343


<Backdoor Training> Train Epoch: 78 	Loss: 0.003839, lr: 0.001000, Time: 6.16s
Clean ACC: 7483/8000 = 0.935375, Loss: 0.2756853699684143
ASR: 2429/7193 = 0.337689


<Backdoor Training> Train Epoch: 79 	Loss: 0.012334, lr: 0.001000, Time: 6.28s
Clean ACC: 7478/8000 = 0.934750, Loss: 0.2745046317577362
ASR: 2400/7193 = 0.333658


<Backdoor Training> Train Epoch: 80 	Loss: 0.004274, lr: 0.001000, Time: 6.10s
Clean ACC: 7481/8000 = 0.935125, Loss: 0.2738083004951477
ASR: 2457/7193 = 0.341582


<Backdoor Training> Train Epoch: 81 	Loss: 0.004735, lr: 0.001000, Time: 6.36s
Clean ACC: 7471/8000 = 0.933875, Loss: 0.2745179235935211
ASR: 2431/7193 = 0.337967


<Backdoor Training> Train Epoch: 82 	Loss: 0.001680, lr: 0.001000, Time: 6.20s
Clean ACC: 7479/8000 = 0.934875, Loss: 0.2765081822872162
ASR: 2599/7193 = 0.361324


<Backdoor Training> Train Epoch: 83 	Loss: 0.000589, lr: 0.001000, Time: 6.79s
Clean ACC: 7478/8000 = 0.934750, Loss: 0.27219367027282715
ASR: 2376/7193 = 0.330321


<Backdoor Training> Train Epoch: 84 	Loss: 0.005805, lr: 0.001000, Time: 6.81s
Clean ACC: 7477/8000 = 0.934625, Loss: 0.2746482193470001
ASR: 2593/7193 = 0.360489


<Backdoor Training> Train Epoch: 85 	Loss: 0.005806, lr: 0.001000, Time: 6.72s
Clean ACC: 7480/8000 = 0.935000, Loss: 0.2727510631084442
ASR: 2350/7193 = 0.326707


<Backdoor Training> Train Epoch: 86 	Loss: 0.001409, lr: 0.001000, Time: 6.81s
Clean ACC: 7479/8000 = 0.934875, Loss: 0.27487242221832275
ASR: 2643/7193 = 0.367441


<Backdoor Training> Train Epoch: 87 	Loss: 0.011440, lr: 0.001000, Time: 6.85s
Clean ACC: 7480/8000 = 0.935000, Loss: 0.2740286886692047
ASR: 2566/7193 = 0.356736


<Backdoor Training> Train Epoch: 88 	Loss: 0.005164, lr: 0.001000, Time: 6.37s
Clean ACC: 7482/8000 = 0.935250, Loss: 0.27449291944503784
ASR: 2434/7193 = 0.338385


<Backdoor Training> Train Epoch: 89 	Loss: 0.010682, lr: 0.001000, Time: 6.22s
Clean ACC: 7479/8000 = 0.934875, Loss: 0.2730206847190857
ASR: 2425/7193 = 0.337133


<Backdoor Training> Train Epoch: 90 	Loss: 0.004349, lr: 0.001000, Time: 6.41s
Clean ACC: 7474/8000 = 0.934250, Loss: 0.27634701132774353
ASR: 2600/7193 = 0.361463


<Backdoor Training> Train Epoch: 91 	Loss: 0.001211, lr: 0.001000, Time: 6.17s
Clean ACC: 7478/8000 = 0.934750, Loss: 0.27567341923713684
ASR: 2500/7193 = 0.347560


<Backdoor Training> Train Epoch: 92 	Loss: 0.025371, lr: 0.001000, Time: 6.24s
Clean ACC: 7478/8000 = 0.934750, Loss: 0.2750732898712158
ASR: 2593/7193 = 0.360489


<Backdoor Training> Train Epoch: 93 	Loss: 0.002048, lr: 0.001000, Time: 6.29s
Clean ACC: 7482/8000 = 0.935250, Loss: 0.2753801643848419
ASR: 2605/7193 = 0.362158


<Backdoor Training> Train Epoch: 94 	Loss: 0.001190, lr: 0.001000, Time: 6.18s
Clean ACC: 7486/8000 = 0.935750, Loss: 0.2735435962677002
ASR: 2473/7193 = 0.343806


<Backdoor Training> Train Epoch: 95 	Loss: 0.007249, lr: 0.001000, Time: 6.78s
Clean ACC: 7473/8000 = 0.934125, Loss: 0.27596792578697205
ASR: 2428/7193 = 0.337550


<Backdoor Training> Train Epoch: 96 	Loss: 0.000676, lr: 0.001000, Time: 6.88s
Clean ACC: 7475/8000 = 0.934375, Loss: 0.27688807249069214
ASR: 2732/7193 = 0.379814


<Backdoor Training> Train Epoch: 97 	Loss: 0.027588, lr: 0.001000, Time: 6.81s
Clean ACC: 7474/8000 = 0.934250, Loss: 0.27479398250579834
ASR: 2428/7193 = 0.337550


<Backdoor Training> Train Epoch: 98 	Loss: 0.002486, lr: 0.001000, Time: 6.88s
Clean ACC: 7484/8000 = 0.935500, Loss: 0.2750633955001831
ASR: 2602/7193 = 0.361741


<Backdoor Training> Train Epoch: 99 	Loss: 0.028250, lr: 0.001000, Time: 6.79s
Clean ACC: 7471/8000 = 0.933875, Loss: 0.27324315905570984
ASR: 2256/7193 = 0.313638


<Backdoor Training> Train Epoch: 100 	Loss: 0.000879, lr: 0.001000, Time: 6.42s
Clean ACC: 7477/8000 = 0.934625, Loss: 0.27457255125045776
ASR: 2498/7193 = 0.347282


Testing the backdoor model...
Evaluating model 'poisoned_train_set/cifar10/WaNet_0.010_cover=0.000_poison_seed=0/full_base_aug_seed=2333.pt'...
Clean ACC: 7477/8000 = 0.934625, Loss: 0.27457255125045776
ASR: 2498/7193 = 0.347282

Visualizing the model's latent space...
Visualizing model 'poisoned_train_set/cifar10/WaNet_0.010_cover=0.000_poison_seed=0/full_base_aug_seed=2333.pt' on cifar10...
[test]
Clean ACC: 7477/8000 = 0.934625, Loss: 0.27457451820373535
ASR: 2499/7193 = 0.347421

Total Clean: 49500
Total Poisoned: 500
torch.Size([512])
clean_dis: 3.319338, poison_dis: 7.338486
Silhouette Score: 0.39543283
Saved figure at assets/pca_cifar10_WaNet_0.010_cover=0.000_poison_seed=0_full_base_aug_seed=2333.pt_class=0.png
Visualizing model 'poisoned_train_set/cifar10/WaNet_0.010_cover=0.000_poison_seed=0/full_base_aug_seed=2333.pt' on cifar10...
[test]
Clean ACC: 7477/8000 = 0.934625, Loss: 0.27457451820373535
ASR: 2499/7193 = 0.347421

Total Clean: 49500
Total Poisoned: 500
torch.Size([512])
clean_dis: 3.319338, poison_dis: 7.338486
Silhouette Score: 0.39543283
Saved figure at assets/tsne_cifar10_WaNet_0.010_cover=0.000_poison_seed=0_full_base_aug_seed=2333.pt_class=0.png
Visualizing model 'poisoned_train_set/cifar10/WaNet_0.010_cover=0.000_poison_seed=0/full_base_aug_seed=2333.pt' on cifar10...
[test]
Clean ACC: 7477/8000 = 0.934625, Loss: 0.27457451820373535
ASR: 2499/7193 = 0.347421

Total Clean: 49500
Total Poisoned: 500
torch.Size([512])
clean_dis: 3.319338, poison_dis: 7.338486
Silhouette Score: 0.39543283
SVM Accuracy: 0.9910091743119266
Saved figure at assets/oracle_cifar10_WaNet_0.010_cover=0.000_poison_seed=0_full_base_aug_seed=2333.pt_class=0.png
Applying IBD_PSC defense method...
trigger_path: ./triggers/none
No trigger mask found! By default masking all black pixels...
Evaluating model 'poisoned_train_set/cifar10/WaNet_0.010_cover=0.000_poison_seed=0/full_base_aug_seed=2333.pt'...
ba: 93.4625015258789
asr: 34.728206634521484
target label: tensor([0], device='cuda:0')
start_index: 10
TPR: 20.32
FPR: 2.86
AUC: 0.5872
f1 score: 0.3299847792998478
Elapsed time: 19.54s
Experiment for cifar10 with WaNet completed.
