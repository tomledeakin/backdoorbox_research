Creating poisoned training set for SIG on cifar10...
[target class : 0]
Files already downloaded and verified
[199, 349, 373, 468, 557, 757, 812, 911, 1266, 1335, 1432, 1434, 1466, 1507, 1540, 1594, 1674, 1708, 1913, 2006, 2107, 2171, 2277, 2429, 2453, 2459, 2473, 2863, 3024, 3049, 3102, 3478, 3570, 3574, 3585, 3644, 3647, 3884, 3940, 3962, 4027, 4061, 4202, 4240, 4249, 4476, 4509, 4516, 4537, 4571, 4577, 4601, 4759, 4861, 4940, 5210, 5323, 5331, 5404, 5702, 5757, 5883, 5888, 5924, 5988, 6054, 6142, 6190, 6370, 6425, 6858, 6867, 6919, 7060, 7093, 7095, 7168, 7174, 7599, 7612, 7842, 7898, 7979, 8023, 8072, 8083, 8132, 8209, 8382, 8425, 8515, 8813, 8953, 9057, 9096, 9119, 9375, 9413, 9438, 9755, 9799, 10024, 10205, 10266, 10297, 10499, 10667, 10675, 10798, 10990, 11014, 11022, 11237, 11251, 11255, 11475, 11615, 11732, 11783, 11849, 11979, 12052, 12130, 12252, 12265, 12363, 12553, 12915, 12991, 12993, 13086, 13106, 13206, 13326, 13473, 13489, 13594, 13800, 13833, 13880, 14079, 14169, 14334, 14350, 14592, 14689, 14949, 15113, 15396, 15436, 15452, 15483, 15541, 15572, 15580, 15923, 16065, 16115, 16182, 16186, 16230, 16420, 16529, 16625, 17100, 17174, 17353, 17458, 17866, 18009, 18012, 18315, 18406, 18505, 18572, 18614, 18839, 18900, 18906, 18925, 18979, 19011, 19084, 19106, 19196, 19216, 19286, 19632, 19664, 19818, 19961, 20039, 20190, 20328, 20355, 20383, 20454, 20606, 20722, 20743, 20759, 20924, 21095, 21158, 21522, 21533, 21761, 21782, 21924, 22159, 22164, 22189, 22249, 22258, 22276, 22408, 22449, 22506, 22665, 22692, 22967, 22993, 23155, 23157, 23171, 23221, 23449, 23746, 23825, 23836, 23863, 23899, 23984, 24365, 24445, 24676, 24681, 24791, 24881, 24931, 25066, 25290, 25337, 25350, 25352, 25362, 25385, 25391, 25457, 25624, 25644, 25668, 25789, 25880, 26040, 26162, 26200, 26376, 26430, 26488, 26667, 26723, 26744, 26815, 26824, 26913, 27007, 27026, 27202, 27425, 27438, 27623, 27710, 27873, 28063, 28179, 28216, 28300, 28311, 28360, 28400, 28512, 28515, 28627, 28919, 28978, 29190, 29257, 29364, 29367, 29518, 29544, 29634, 29653, 29666, 29748, 29754, 29783, 29837, 29879, 29884, 30141, 30257, 30517, 30539, 30914, 31004, 31083, 31164, 31674, 31693, 32002, 32149, 32251, 32267, 32378, 32522, 32761, 32934, 33000, 33117, 33130, 33186, 33214, 33249, 33347, 33360, 33451, 33489, 33542, 33584, 33654, 33670, 33923, 33933, 34012, 34109, 34121, 34131, 34434, 34664, 34676, 34693, 34724, 34745, 34748, 34780, 35179, 35195, 35209, 35524, 35544, 35546, 35731, 35782, 35798, 35820, 35873, 35927, 35952, 36038, 36068, 36100, 36119, 36155, 36252, 36312, 36337, 36386, 36530, 36610, 36927, 36981, 37063, 37078, 37177, 37240, 37439, 37493, 37646, 37738, 37872, 37949, 37978, 38198, 38285, 38321, 38519, 38566, 38716, 38937, 39006, 39026, 39080, 39121, 39201, 39281, 39286, 39392, 39409, 39447, 39716, 39723, 39819, 39877, 39959, 40287, 40401, 40432, 40619, 40648, 40945, 41030, 41121, 41212, 41243, 41574, 41664, 41821, 41855, 41868, 42103, 42190, 42201, 42435, 42439, 42618, 42722, 42867, 42878, 42945, 43160, 43211, 43219, 43442, 43454, 43525, 43564, 43667, 43727, 43750, 43944, 43947, 43998, 44037, 44116, 44165, 44245, 44454, 44464, 44498, 44645, 44653, 44743, 44806, 44866, 45123, 45163, 45357, 45656, 45666, 46192, 46200, 46366, 46367, 46394, 46576, 46603, 46631, 46655, 46756, 46802, 46922, 46992, 47320, 47332, 47336, 47676, 47685, 47693, 47711, 47726, 47834, 47925, 48024, 48174, 48266, 48327, 48428, 48489, 48492, 48520, 48580, 48792, 48986, 49100, 49164, 49339, 49510, 49530]
[Generate Poisoned Set] Save 50000 Images
[Generate Poisoned Set] Save poisoned_train_set/cifar10/SIG_0.010_poison_seed=0/imgs
[Generate Poisoned Set] Save poisoned_train_set/cifar10/SIG_0.010_poison_seed=0/labels
[Generate Poisoned Set] Save poisoned_train_set/cifar10/SIG_0.010_poison_seed=0/poison_indices
Training the model on the poisoned dataset...
dataset : poisoned_train_set/cifar10/SIG_0.010_poison_seed=0/imgs
Will save to 'poisoned_train_set/cifar10/SIG_0.010_poison_seed=0/full_base_aug_seed=2333.pt'.
<Backdoor Training> Train Epoch: 1 	Loss: 1.623552, lr: 0.100000, Time: 6.71s
Clean ACC: 3503/8000 = 0.437875, Loss: 1.527016520500183
ASR: 196/7193 = 0.027249


<Backdoor Training> Train Epoch: 2 	Loss: 1.104224, lr: 0.100000, Time: 6.22s
Clean ACC: 4246/8000 = 0.530750, Loss: 1.2947847843170166
ASR: 771/7193 = 0.107188


<Backdoor Training> Train Epoch: 3 	Loss: 1.011803, lr: 0.100000, Time: 6.78s
Clean ACC: 5072/8000 = 0.634000, Loss: 1.0311583280563354
ASR: 3742/7193 = 0.520228


<Backdoor Training> Train Epoch: 4 	Loss: 0.999233, lr: 0.100000, Time: 6.85s
Clean ACC: 5360/8000 = 0.670000, Loss: 0.9539965391159058
ASR: 4677/7193 = 0.650215


<Backdoor Training> Train Epoch: 5 	Loss: 0.570277, lr: 0.100000, Time: 6.66s
Clean ACC: 5670/8000 = 0.708750, Loss: 0.8392780423164368
ASR: 6412/7193 = 0.891422


<Backdoor Training> Train Epoch: 6 	Loss: 0.743630, lr: 0.100000, Time: 6.76s
Clean ACC: 5732/8000 = 0.716500, Loss: 0.8308220505714417
ASR: 1368/7193 = 0.190185


<Backdoor Training> Train Epoch: 7 	Loss: 0.730610, lr: 0.100000, Time: 6.61s
Clean ACC: 6229/8000 = 0.778625, Loss: 0.6507552266120911
ASR: 3027/7193 = 0.420826


<Backdoor Training> Train Epoch: 8 	Loss: 0.582786, lr: 0.100000, Time: 6.33s
Clean ACC: 6328/8000 = 0.791000, Loss: 0.6303619742393494
ASR: 4330/7193 = 0.601974


<Backdoor Training> Train Epoch: 9 	Loss: 0.415255, lr: 0.100000, Time: 6.34s
Clean ACC: 6492/8000 = 0.811500, Loss: 0.554689884185791
ASR: 3402/7193 = 0.472960


<Backdoor Training> Train Epoch: 10 	Loss: 0.680615, lr: 0.100000, Time: 6.37s
Clean ACC: 6639/8000 = 0.829875, Loss: 0.4977850019931793
ASR: 3735/7193 = 0.519255


<Backdoor Training> Train Epoch: 11 	Loss: 0.213418, lr: 0.100000, Time: 6.12s
Clean ACC: 6587/8000 = 0.823375, Loss: 0.5369248986244202
ASR: 6021/7193 = 0.837064


<Backdoor Training> Train Epoch: 12 	Loss: 0.379945, lr: 0.100000, Time: 6.51s
Clean ACC: 6782/8000 = 0.847750, Loss: 0.45603442192077637
ASR: 4292/7193 = 0.596691


<Backdoor Training> Train Epoch: 13 	Loss: 0.449946, lr: 0.100000, Time: 6.80s
Clean ACC: 6622/8000 = 0.827750, Loss: 0.5103299021720886
ASR: 4266/7193 = 0.593077


<Backdoor Training> Train Epoch: 14 	Loss: 0.346818, lr: 0.100000, Time: 6.82s
Clean ACC: 6904/8000 = 0.863000, Loss: 0.4022752046585083
ASR: 4049/7193 = 0.562908


<Backdoor Training> Train Epoch: 15 	Loss: 0.379731, lr: 0.100000, Time: 6.74s
Clean ACC: 6710/8000 = 0.838750, Loss: 0.4808140695095062
ASR: 4351/7193 = 0.604894


<Backdoor Training> Train Epoch: 16 	Loss: 0.301910, lr: 0.100000, Time: 6.55s
Clean ACC: 6849/8000 = 0.856125, Loss: 0.43274253606796265
ASR: 4696/7193 = 0.652857


<Backdoor Training> Train Epoch: 17 	Loss: 0.337719, lr: 0.100000, Time: 6.43s
Clean ACC: 6781/8000 = 0.847625, Loss: 0.4861694574356079
ASR: 4163/7193 = 0.578757


<Backdoor Training> Train Epoch: 18 	Loss: 0.360028, lr: 0.100000, Time: 6.33s
Clean ACC: 6981/8000 = 0.872625, Loss: 0.38965868949890137
ASR: 5293/7193 = 0.735854


<Backdoor Training> Train Epoch: 19 	Loss: 0.125489, lr: 0.100000, Time: 6.35s
Clean ACC: 6930/8000 = 0.866250, Loss: 0.43450164794921875
ASR: 6038/7193 = 0.839427


<Backdoor Training> Train Epoch: 20 	Loss: 0.272414, lr: 0.100000, Time: 6.26s
Clean ACC: 6900/8000 = 0.862500, Loss: 0.42211973667144775
ASR: 5125/7193 = 0.712498


<Backdoor Training> Train Epoch: 21 	Loss: 0.257422, lr: 0.100000, Time: 6.36s
Clean ACC: 7059/8000 = 0.882375, Loss: 0.35034114122390747
ASR: 5706/7193 = 0.793271


<Backdoor Training> Train Epoch: 22 	Loss: 0.250693, lr: 0.100000, Time: 6.44s
Clean ACC: 6840/8000 = 0.855000, Loss: 0.45246797800064087
ASR: 3207/7193 = 0.445850


<Backdoor Training> Train Epoch: 23 	Loss: 0.200505, lr: 0.100000, Time: 6.69s
Clean ACC: 7068/8000 = 0.883500, Loss: 0.3577626645565033
ASR: 5846/7193 = 0.812735


<Backdoor Training> Train Epoch: 24 	Loss: 0.152287, lr: 0.100000, Time: 6.67s
Clean ACC: 7160/8000 = 0.895000, Loss: 0.316679447889328
ASR: 4572/7193 = 0.635618


<Backdoor Training> Train Epoch: 25 	Loss: 0.155592, lr: 0.100000, Time: 6.65s
Clean ACC: 6906/8000 = 0.863250, Loss: 0.45479264855384827
ASR: 3723/7193 = 0.517587


<Backdoor Training> Train Epoch: 26 	Loss: 0.216566, lr: 0.100000, Time: 6.65s
Clean ACC: 7032/8000 = 0.879000, Loss: 0.38027241826057434
ASR: 2777/7193 = 0.386070


<Backdoor Training> Train Epoch: 27 	Loss: 0.120240, lr: 0.100000, Time: 6.47s
Clean ACC: 6980/8000 = 0.872500, Loss: 0.4121147096157074
ASR: 4341/7193 = 0.603503


<Backdoor Training> Train Epoch: 28 	Loss: 0.197108, lr: 0.100000, Time: 6.31s
Clean ACC: 7045/8000 = 0.880625, Loss: 0.3600575923919678
ASR: 4146/7193 = 0.576394


<Backdoor Training> Train Epoch: 29 	Loss: 0.262403, lr: 0.100000, Time: 6.40s
Clean ACC: 7100/8000 = 0.887500, Loss: 0.36536404490470886
ASR: 4496/7193 = 0.625052


<Backdoor Training> Train Epoch: 30 	Loss: 0.131204, lr: 0.100000, Time: 6.30s
Clean ACC: 7198/8000 = 0.899750, Loss: 0.3281671702861786
ASR: 5621/7193 = 0.781454


<Backdoor Training> Train Epoch: 31 	Loss: 0.356181, lr: 0.100000, Time: 6.37s
Clean ACC: 7067/8000 = 0.883375, Loss: 0.37474584579467773
ASR: 3995/7193 = 0.555401


<Backdoor Training> Train Epoch: 32 	Loss: 0.145573, lr: 0.100000, Time: 6.66s
Clean ACC: 6970/8000 = 0.871250, Loss: 0.42047005891799927
ASR: 4842/7193 = 0.673154


<Backdoor Training> Train Epoch: 33 	Loss: 0.162659, lr: 0.100000, Time: 6.56s
Clean ACC: 7101/8000 = 0.887625, Loss: 0.3477932810783386
ASR: 2956/7193 = 0.410955


<Backdoor Training> Train Epoch: 34 	Loss: 0.161145, lr: 0.100000, Time: 6.73s
Clean ACC: 7089/8000 = 0.886125, Loss: 0.36854538321495056
ASR: 4305/7193 = 0.598499


<Backdoor Training> Train Epoch: 35 	Loss: 0.027482, lr: 0.100000, Time: 6.81s
Clean ACC: 6990/8000 = 0.873750, Loss: 0.4228709638118744
ASR: 4881/7193 = 0.678576


<Backdoor Training> Train Epoch: 36 	Loss: 0.282758, lr: 0.100000, Time: 6.77s
Clean ACC: 7038/8000 = 0.879750, Loss: 0.4135606288909912
ASR: 5001/7193 = 0.695259


<Backdoor Training> Train Epoch: 37 	Loss: 0.177591, lr: 0.100000, Time: 6.25s
Clean ACC: 6979/8000 = 0.872375, Loss: 0.4512031376361847
ASR: 4347/7193 = 0.604338


<Backdoor Training> Train Epoch: 38 	Loss: 0.135977, lr: 0.100000, Time: 6.42s
Clean ACC: 7098/8000 = 0.887250, Loss: 0.35387367010116577
ASR: 3573/7193 = 0.496733


<Backdoor Training> Train Epoch: 39 	Loss: 0.179763, lr: 0.100000, Time: 6.37s
Clean ACC: 7180/8000 = 0.897500, Loss: 0.3432236611843109
ASR: 6067/7193 = 0.843459


<Backdoor Training> Train Epoch: 40 	Loss: 0.059721, lr: 0.100000, Time: 6.19s
Clean ACC: 7048/8000 = 0.881000, Loss: 0.3857539892196655
ASR: 4482/7193 = 0.623106


<Backdoor Training> Train Epoch: 41 	Loss: 0.109372, lr: 0.100000, Time: 6.25s
Clean ACC: 7014/8000 = 0.876750, Loss: 0.42241740226745605
ASR: 5602/7193 = 0.778813


<Backdoor Training> Train Epoch: 42 	Loss: 0.073613, lr: 0.100000, Time: 6.73s
Clean ACC: 7200/8000 = 0.900000, Loss: 0.33781328797340393
ASR: 5106/7193 = 0.709857


<Backdoor Training> Train Epoch: 43 	Loss: 0.176738, lr: 0.100000, Time: 6.67s
Clean ACC: 7206/8000 = 0.900750, Loss: 0.33499112725257874
ASR: 4861/7193 = 0.675796


<Backdoor Training> Train Epoch: 44 	Loss: 0.179929, lr: 0.100000, Time: 6.65s
Clean ACC: 7165/8000 = 0.895625, Loss: 0.3524678945541382
ASR: 5715/7193 = 0.794522


<Backdoor Training> Train Epoch: 45 	Loss: 0.138984, lr: 0.100000, Time: 6.61s
Clean ACC: 7122/8000 = 0.890250, Loss: 0.3688695430755615
ASR: 4053/7193 = 0.563464


<Backdoor Training> Train Epoch: 46 	Loss: 0.183974, lr: 0.100000, Time: 6.63s
Clean ACC: 7084/8000 = 0.885500, Loss: 0.3823030889034271
ASR: 4570/7193 = 0.635340


<Backdoor Training> Train Epoch: 47 	Loss: 0.039019, lr: 0.100000, Time: 6.22s
Clean ACC: 7108/8000 = 0.888500, Loss: 0.3952934145927429
ASR: 4896/7193 = 0.680662


<Backdoor Training> Train Epoch: 48 	Loss: 0.211136, lr: 0.100000, Time: 6.10s
Clean ACC: 7104/8000 = 0.888000, Loss: 0.375302255153656
ASR: 4412/7193 = 0.613374


<Backdoor Training> Train Epoch: 49 	Loss: 0.048848, lr: 0.100000, Time: 6.26s
Clean ACC: 7184/8000 = 0.898000, Loss: 0.34347596764564514
ASR: 4861/7193 = 0.675796


<Backdoor Training> Train Epoch: 50 	Loss: 0.087071, lr: 0.100000, Time: 6.20s
Clean ACC: 7198/8000 = 0.899750, Loss: 0.3361988067626953
ASR: 5187/7193 = 0.721118


<Backdoor Training> Train Epoch: 51 	Loss: 0.051831, lr: 0.010000, Time: 6.27s
Clean ACC: 7463/8000 = 0.932875, Loss: 0.22689008712768555
ASR: 5108/7193 = 0.710135


<Backdoor Training> Train Epoch: 52 	Loss: 0.045287, lr: 0.010000, Time: 6.61s
Clean ACC: 7453/8000 = 0.931625, Loss: 0.2297593653202057
ASR: 5186/7193 = 0.720979


<Backdoor Training> Train Epoch: 53 	Loss: 0.018089, lr: 0.010000, Time: 6.71s
Clean ACC: 7471/8000 = 0.933875, Loss: 0.22990615665912628
ASR: 5175/7193 = 0.719449


<Backdoor Training> Train Epoch: 54 	Loss: 0.009118, lr: 0.010000, Time: 6.74s
Clean ACC: 7471/8000 = 0.933875, Loss: 0.23325881361961365
ASR: 5337/7193 = 0.741971


<Backdoor Training> Train Epoch: 55 	Loss: 0.015068, lr: 0.010000, Time: 6.70s
Clean ACC: 7480/8000 = 0.935000, Loss: 0.23613952100276947
ASR: 5178/7193 = 0.719867


<Backdoor Training> Train Epoch: 56 	Loss: 0.035402, lr: 0.010000, Time: 6.69s
Clean ACC: 7472/8000 = 0.934000, Loss: 0.23392373323440552
ASR: 4918/7193 = 0.683720


<Backdoor Training> Train Epoch: 57 	Loss: 0.005625, lr: 0.010000, Time: 6.21s
Clean ACC: 7472/8000 = 0.934000, Loss: 0.2412024289369583
ASR: 5197/7193 = 0.722508


<Backdoor Training> Train Epoch: 58 	Loss: 0.004412, lr: 0.010000, Time: 6.25s
Clean ACC: 7483/8000 = 0.935375, Loss: 0.23960238695144653
ASR: 5256/7193 = 0.730710


<Backdoor Training> Train Epoch: 59 	Loss: 0.024926, lr: 0.010000, Time: 6.29s
Clean ACC: 7482/8000 = 0.935250, Loss: 0.24349810183048248
ASR: 5064/7193 = 0.704018


<Backdoor Training> Train Epoch: 60 	Loss: 0.017383, lr: 0.010000, Time: 6.34s
Clean ACC: 7484/8000 = 0.935500, Loss: 0.2483808547258377
ASR: 5004/7193 = 0.695676


<Backdoor Training> Train Epoch: 61 	Loss: 0.014872, lr: 0.010000, Time: 6.53s
Clean ACC: 7489/8000 = 0.936125, Loss: 0.2484811693429947
ASR: 5395/7193 = 0.750035


<Backdoor Training> Train Epoch: 62 	Loss: 0.002181, lr: 0.010000, Time: 6.70s
Clean ACC: 7493/8000 = 0.936625, Loss: 0.24662594497203827
ASR: 5051/7193 = 0.702210


<Backdoor Training> Train Epoch: 63 	Loss: 0.021101, lr: 0.010000, Time: 6.79s
Clean ACC: 7505/8000 = 0.938125, Loss: 0.2489490807056427
ASR: 4820/7193 = 0.670096


<Backdoor Training> Train Epoch: 64 	Loss: 0.002972, lr: 0.010000, Time: 6.74s
Clean ACC: 7487/8000 = 0.935875, Loss: 0.25226014852523804
ASR: 5090/7193 = 0.707632


<Backdoor Training> Train Epoch: 65 	Loss: 0.007713, lr: 0.010000, Time: 6.80s
Clean ACC: 7489/8000 = 0.936125, Loss: 0.25607138872146606
ASR: 4911/7193 = 0.682747


<Backdoor Training> Train Epoch: 66 	Loss: 0.015445, lr: 0.010000, Time: 6.55s
Clean ACC: 7498/8000 = 0.937250, Loss: 0.25046467781066895
ASR: 4859/7193 = 0.675518


<Backdoor Training> Train Epoch: 67 	Loss: 0.030574, lr: 0.010000, Time: 6.31s
Clean ACC: 7483/8000 = 0.935375, Loss: 0.2555527687072754
ASR: 5032/7193 = 0.699569


<Backdoor Training> Train Epoch: 68 	Loss: 0.013070, lr: 0.010000, Time: 6.36s
Clean ACC: 7504/8000 = 0.938000, Loss: 0.25546833872795105
ASR: 5162/7193 = 0.717642


<Backdoor Training> Train Epoch: 69 	Loss: 0.001429, lr: 0.010000, Time: 6.25s
Clean ACC: 7496/8000 = 0.937000, Loss: 0.2601470649242401
ASR: 5169/7193 = 0.718615


<Backdoor Training> Train Epoch: 70 	Loss: 0.011879, lr: 0.010000, Time: 6.15s
Clean ACC: 7504/8000 = 0.938000, Loss: 0.25914424657821655
ASR: 4949/7193 = 0.688030


<Backdoor Training> Train Epoch: 71 	Loss: 0.010257, lr: 0.010000, Time: 6.42s
Clean ACC: 7497/8000 = 0.937125, Loss: 0.2615286707878113
ASR: 5112/7193 = 0.710691


<Backdoor Training> Train Epoch: 72 	Loss: 0.002328, lr: 0.010000, Time: 6.80s
Clean ACC: 7501/8000 = 0.937625, Loss: 0.26001232862472534
ASR: 5193/7193 = 0.721952


<Backdoor Training> Train Epoch: 73 	Loss: 0.006146, lr: 0.010000, Time: 6.71s
Clean ACC: 7497/8000 = 0.937125, Loss: 0.263710081577301
ASR: 4953/7193 = 0.688586


<Backdoor Training> Train Epoch: 74 	Loss: 0.003167, lr: 0.010000, Time: 6.63s
Clean ACC: 7513/8000 = 0.939125, Loss: 0.2601540982723236
ASR: 5126/7193 = 0.712637


<Backdoor Training> Train Epoch: 75 	Loss: 0.007104, lr: 0.010000, Time: 6.73s
Clean ACC: 7499/8000 = 0.937375, Loss: 0.2670082449913025
ASR: 4994/7193 = 0.694286


<Backdoor Training> Train Epoch: 76 	Loss: 0.002091, lr: 0.001000, Time: 6.35s
Clean ACC: 7494/8000 = 0.936750, Loss: 0.26265889406204224
ASR: 5112/7193 = 0.710691


<Backdoor Training> Train Epoch: 77 	Loss: 0.012113, lr: 0.001000, Time: 6.37s
Clean ACC: 7494/8000 = 0.936750, Loss: 0.2651244103908539
ASR: 4965/7193 = 0.690254


<Backdoor Training> Train Epoch: 78 	Loss: 0.005549, lr: 0.001000, Time: 6.42s
Clean ACC: 7488/8000 = 0.936000, Loss: 0.2629276216030121
ASR: 5135/7193 = 0.713889


<Backdoor Training> Train Epoch: 79 	Loss: 0.000339, lr: 0.001000, Time: 6.24s
Clean ACC: 7501/8000 = 0.937625, Loss: 0.26207125186920166
ASR: 5089/7193 = 0.707493


<Backdoor Training> Train Epoch: 80 	Loss: 0.001517, lr: 0.001000, Time: 6.30s
Clean ACC: 7499/8000 = 0.937375, Loss: 0.26321107149124146
ASR: 5092/7193 = 0.707910


<Backdoor Training> Train Epoch: 81 	Loss: 0.033470, lr: 0.001000, Time: 6.58s
Clean ACC: 7506/8000 = 0.938250, Loss: 0.2651570737361908
ASR: 5120/7193 = 0.711803


<Backdoor Training> Train Epoch: 82 	Loss: 0.002481, lr: 0.001000, Time: 6.72s
Clean ACC: 7493/8000 = 0.936625, Loss: 0.2625240087509155
ASR: 5015/7193 = 0.697206


<Backdoor Training> Train Epoch: 83 	Loss: 0.001344, lr: 0.001000, Time: 6.66s
Clean ACC: 7505/8000 = 0.938125, Loss: 0.26120343804359436
ASR: 5159/7193 = 0.717225


<Backdoor Training> Train Epoch: 84 	Loss: 0.001672, lr: 0.001000, Time: 6.65s
Clean ACC: 7500/8000 = 0.937500, Loss: 0.26241180300712585
ASR: 5101/7193 = 0.709162


<Backdoor Training> Train Epoch: 85 	Loss: 0.000956, lr: 0.001000, Time: 6.73s
Clean ACC: 7499/8000 = 0.937375, Loss: 0.26274338364601135
ASR: 5133/7193 = 0.713610


<Backdoor Training> Train Epoch: 86 	Loss: 0.001505, lr: 0.001000, Time: 6.32s
Clean ACC: 7499/8000 = 0.937375, Loss: 0.2611554265022278
ASR: 5229/7193 = 0.726957


<Backdoor Training> Train Epoch: 87 	Loss: 0.006150, lr: 0.001000, Time: 6.21s
Clean ACC: 7504/8000 = 0.938000, Loss: 0.26319649815559387
ASR: 5030/7193 = 0.699291


<Backdoor Training> Train Epoch: 88 	Loss: 0.000596, lr: 0.001000, Time: 6.31s
Clean ACC: 7507/8000 = 0.938375, Loss: 0.262004017829895
ASR: 4859/7193 = 0.675518


<Backdoor Training> Train Epoch: 89 	Loss: 0.003754, lr: 0.001000, Time: 6.21s
Clean ACC: 7512/8000 = 0.939000, Loss: 0.2630622386932373
ASR: 5063/7193 = 0.703879


<Backdoor Training> Train Epoch: 90 	Loss: 0.002354, lr: 0.001000, Time: 6.24s
Clean ACC: 7502/8000 = 0.937750, Loss: 0.26126691699028015
ASR: 4975/7193 = 0.691645


<Backdoor Training> Train Epoch: 91 	Loss: 0.003331, lr: 0.001000, Time: 6.73s
Clean ACC: 7509/8000 = 0.938625, Loss: 0.2622146010398865
ASR: 4949/7193 = 0.688030


<Backdoor Training> Train Epoch: 92 	Loss: 0.004049, lr: 0.001000, Time: 6.67s
Clean ACC: 7503/8000 = 0.937875, Loss: 0.2612973153591156
ASR: 5108/7193 = 0.710135


<Backdoor Training> Train Epoch: 93 	Loss: 0.002042, lr: 0.001000, Time: 6.59s
Clean ACC: 7512/8000 = 0.939000, Loss: 0.26061034202575684
ASR: 5089/7193 = 0.707493


<Backdoor Training> Train Epoch: 94 	Loss: 0.001896, lr: 0.001000, Time: 6.71s
Clean ACC: 7501/8000 = 0.937625, Loss: 0.2612515985965729
ASR: 5090/7193 = 0.707632


<Backdoor Training> Train Epoch: 95 	Loss: 0.000615, lr: 0.001000, Time: 6.58s
Clean ACC: 7505/8000 = 0.938125, Loss: 0.26207152009010315
ASR: 5182/7193 = 0.720423


<Backdoor Training> Train Epoch: 96 	Loss: 0.001792, lr: 0.001000, Time: 6.31s
Clean ACC: 7506/8000 = 0.938250, Loss: 0.26284515857696533
ASR: 5041/7193 = 0.700820


<Backdoor Training> Train Epoch: 97 	Loss: 0.033997, lr: 0.001000, Time: 6.24s
Clean ACC: 7500/8000 = 0.937500, Loss: 0.2631838619709015
ASR: 4901/7193 = 0.681357


<Backdoor Training> Train Epoch: 98 	Loss: 0.004220, lr: 0.001000, Time: 6.24s
Clean ACC: 7494/8000 = 0.936750, Loss: 0.2646937966346741
ASR: 5255/7193 = 0.730571


<Backdoor Training> Train Epoch: 99 	Loss: 0.002517, lr: 0.001000, Time: 6.10s
Clean ACC: 7508/8000 = 0.938500, Loss: 0.26073333621025085
ASR: 4964/7193 = 0.690115


<Backdoor Training> Train Epoch: 100 	Loss: 0.007717, lr: 0.001000, Time: 6.51s
Clean ACC: 7503/8000 = 0.937875, Loss: 0.2637667655944824
ASR: 4989/7193 = 0.693591


Testing the backdoor model...
Evaluating model 'poisoned_train_set/cifar10/SIG_0.010_poison_seed=0/full_base_aug_seed=2333.pt'...
Clean ACC: 7503/8000 = 0.937875, Loss: 0.2637667655944824
ASR: 4989/7193 = 0.693591

Visualizing the model's latent space...
Visualizing model 'poisoned_train_set/cifar10/SIG_0.010_poison_seed=0/full_base_aug_seed=2333.pt' on cifar10...
[test]
Clean ACC: 7504/8000 = 0.938000, Loss: 0.26376113295555115
ASR: 4989/7193 = 0.693591

Total Clean: 49500
Total Poisoned: 500
torch.Size([512])
clean_dis: 3.240789, poison_dis: 5.167502
Silhouette Score: 0.26955673
Saved figure at assets/pca_cifar10_SIG_0.010_poison_seed=0_full_base_aug_seed=2333.pt_class=0.png
Visualizing model 'poisoned_train_set/cifar10/SIG_0.010_poison_seed=0/full_base_aug_seed=2333.pt' on cifar10...
[test]
Clean ACC: 7504/8000 = 0.938000, Loss: 0.26376113295555115
ASR: 4989/7193 = 0.693591

Total Clean: 49500
Total Poisoned: 500
torch.Size([512])
clean_dis: 3.240789, poison_dis: 5.167502
Silhouette Score: 0.26955673
Saved figure at assets/tsne_cifar10_SIG_0.010_poison_seed=0_full_base_aug_seed=2333.pt_class=0.png
Visualizing model 'poisoned_train_set/cifar10/SIG_0.010_poison_seed=0/full_base_aug_seed=2333.pt' on cifar10...
[test]
Clean ACC: 7504/8000 = 0.938000, Loss: 0.26376113295555115
ASR: 4989/7193 = 0.693591

Total Clean: 49500
Total Poisoned: 500
torch.Size([512])
clean_dis: 3.240789, poison_dis: 5.167502
Silhouette Score: 0.26955673
SVM Accuracy: 1.0
Saved figure at assets/oracle_cifar10_SIG_0.010_poison_seed=0_full_base_aug_seed=2333.pt_class=0.png
Applying IBD_PSC defense method...
trigger_path: ./triggers/none
No trigger mask found! By default masking all black pixels...
Evaluating model 'poisoned_train_set/cifar10/SIG_0.010_poison_seed=0/full_base_aug_seed=2333.pt'...
ba: 93.78750610351562
asr: 69.35910034179688
target label: tensor([0], device='cuda:0')
start_index: 12
TPR: 0.75
FPR: 4.08
AUC: 0.4243
f1 score: 0.01430956355831147
Elapsed time: 21.21s
Experiment for cifar10 with SIG completed.
